# ğŸ“š Explicar un concepto con un modelo local y guardarlo markdown

Genera una explicaciÃ³n profesional y estructurada sobre cualquier tÃ©rmino o concepto en formato markdown, utilizando Ãºnicamente modelos de inteligencia artificial locales. El sistema no depende de servicios externos: el contenido es generado por IA, aportando una plantilla clara y homogÃ©nea del propio concepto como resultado final. Ideal para herramientas como Obsidian.

## âœ¨ CaracterÃ­sticas principales

- ğŸ¤– **IA local**: Utiliza modelos Ollama (por defecto: gemma3:latest, configurable)
- ğŸ“„ **Formato estructurado**: Documentos Markdown con secciones acadÃ©micas y emojis temÃ¡ticos
- âš¡ **Procesamiento mÃºltiple**: Genera documentaciÃ³n para uno o varios tÃ©rminos en una sola ejecuciÃ³n
- â±ï¸ **MediciÃ³n de tiempo**: Muestra tiempos de generaciÃ³n por tÃ©rmino y total
- ğŸ“ **Sin dependencias externas**: No requiere conexiÃ³n a internet para generar el contenido, solamente ollama y un modelo.

## ğŸ“‹ Estructura de los documentos generados

Cada archivo `.md` sigue una plantilla profesional y homogÃ©nea:

1. **ğŸ“– DefiniciÃ³n**: DefiniciÃ³n acadÃ©mica y clara
2. **ğŸ” CaracterÃ­sticas principales**: 4 aspectos fundamentales
3. **ğŸ“‚ ClasificaciÃ³n y tipos**: Ramas y subdisciplinas
4. **ğŸ¯ Aplicaciones y mÃ©todos**: MÃ©todos y aplicaciones prÃ¡cticas
5. **ğŸ”— Conceptos relacionados**: TÃ©rminos y relaciones clave
6. **ğŸ’¡ Resumen**: SÃ­ntesis y puntos destacados

**Formato:**

- Markdown profesional, con emojis solo en encabezados
- Texto limpio, sin referencias ni sÃ­mbolos extraÃ±os
- Metadatos: fecha y etiqueta de IA

## ğŸ› ï¸ Requisitos del sistema

- **Python 3.8+**
- **Ollama** instalado y ejecutÃ¡ndose localmente (Ollama es imprescindible para realizar inferencia sobre un modelo descargado)
- Al menos un modelo descargado en Ollama (por defecto: gemma3:latest)

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar o descargar el proyecto

```bash
git clone <url-del-repositorio>
cd node-criteria
```

### 2. Instalar dependencias de Python

```bash
pip install -r requirements.txt
```

### 3. Instalar y configurar Ollama

Ollama es imprescindible para poder ejecutar modelos de IA locales y realizar inferencia. Si no tienes Ollama:

1. DescÃ¡rgalo desde [ollama.ai](https://ollama.ai) e instÃ¡lalo en tu sistema.
2. Si necesitas ayuda, sigue este tutorial paso a paso para Windows: [GuÃ­a de instalaciÃ³n de Ollama](https://www.geeknetic.es/Guia/2957/Ollama-Como-usar-LLM-de-IA-locales-desde-Windows.html)
3. Descarga al menos un modelo compatible (por ejemplo):

```bash
ollama pull gemma3:latest
```

### 4. Verificar instalaciÃ³n

```bash
ollama list
```

## ğŸš€ Uso

### Uso bÃ¡sico - Un concepto

```bash
python main.py "tu concepto"
```

### Varios conceptos a la vez

```bash
python main.py "un concepto, otro concepto, otro concepto mÃ¡s"
```

### Ejemplos de tÃ©rminos probados:

- **CientÃ­ficos**: `apoptosis`, `criptografÃ­a`, `inteligencia artificial`
- **PolÃ­ticos**: `liberalismo`, `democracia`
- **TecnolÃ³gicos**: `blockchain`, `fotografÃ­a digital`
- **PsicolÃ³gicos**: `narcisismo`, `empatÃ­a`

## âš™ï¸ ConfiguraciÃ³n

Puedes cambiar el modelo de IA por defecto editando `config.py`:

```python
DEFAULT_MODEL = "gemma3:latest"  # Modelo por defecto
# DEFAULT_MODEL = "phi3:mini"
# DEFAULT_MODEL = "gemma:2b-instruct-q2_K"
```

El directorio de salida tambiÃ©n es configurable:

```python
OUTPUT_DIR = "output"
```

## ğŸ“ Estructura del proyecto

```
node-criteria/
â”œâ”€â”€ main.py              # Script principal ejecutable
â”œâ”€â”€ config.py            # ConfiguraciÃ³n del sistema
â”œâ”€â”€ requirements.txt     # Dependencias de Python
â”œâ”€â”€ README.md            # Este archivo
â”œâ”€â”€ src/                 # CÃ³digo fuente
â”‚   â”œâ”€â”€ doc_generator.py      # Generador principal
â”‚   â””â”€â”€ content_enhancer.py   # IntegraciÃ³n IA
â””â”€â”€ output/              # Documentos generados (.md)
```

## ğŸ“Š Rendimiento

El tiempo de generaciÃ³n depende del modelo, el hardware y la longitud de la respuesta. En mi equipo, los tiempos estimados son:

- **TÃ©rminos cientÃ­ficos**: 90-180 segundos
- **TÃ©rminos generales**: 60-120 segundos

**Nota:** El rendimiento real puede variar significativamente segÃºn la mÃ¡quina y los recursos disponibles de cada usuario.

**Factores:**

- ğŸ–¥ï¸ CPU/RAM disponible
- ğŸ¤– Modelo de IA elegido

## ğŸ¯ Ejemplo de salida

```bash
ğŸš€ Generador de DocumentaciÃ³n con IA
========================================
ğŸ“ TÃ©rmino: apoptosis

ğŸ“ Creando documentaciÃ³n para: apoptosis
ğŸ“„ Archivo: output\apoptosis.md
ğŸ¤– Generando contenido con IA...
âœ… ConexiÃ³n con IA establecida - Modelo: gemma3:latest
ğŸ¤– Generando contenido con gemma3:latest...
âœ… Contenido generado exitosamente (6321 caracteres)
ğŸ‰ DocumentaciÃ³n completa generada en: output\apoptosis.md

ğŸ‰ Â¡DocumentaciÃ³n generada!
ğŸ“„ Archivo: output\apoptosis.md
â±ï¸ Tiempo: 120.60 segundos
```

## ğŸ› ï¸ Desarrollo

### Clases principales

- `DocumentationGenerator`: Genera y guarda la documentaciÃ³n en formato markdown
- `ContentEnhancer`: Interfaz con Ollama para generaciÃ³n de contenido con IA

### PersonalizaciÃ³n del estilo de la respuesta

Si deseas modificar el estilo, la estructura o el tono de las respuestas generadas, puedes hacerlo editando el prompt en el archivo:

- `src/content_enhancer.py` â†’ mÃ©todo `enhance_summary()`

AllÃ­ puedes ajustar la plantilla, los encabezados, la longitud, el nivel de detalle o cualquier instrucciÃ³n para el modelo de IA segÃºn tus preferencias.

### Agregar nuevos modelos

1. Descargar con Ollama: `ollama pull <modelo>`
2. AÃ±adir el nombre del modelo que se quiere usar en `config.py`
3. Ejecutar el script con el modelo deseado

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "Error conectando con Ollama"

```bash
# Verifica que Ollama estÃ© ejecutÃ¡ndose
ollama list
# Si no estÃ¡ ejecutÃ¡ndose, inicia el servicio:
ollama serve
```

### Error: "MÃ³dulos de IA no disponibles"

```bash
pip install -r requirements.txt
```

### Documentos vacÃ­os o incompletos

- Ollama debe estar en funcionamiento
- Prueba con otro modelo
- Revisa que el modelo estÃ© completamente descargado

## ğŸ™ Agradecimientos

- **Ollama** por la infraestructura de modelos e inferencia
- **Microsoft Phi-3** y **Google Gemma** por los modelos de IA
- Comunidad open-source por las librerÃ­as utilizadas

---

## LICENSE

<p align="center">
    Repositorio generado por <a href="https://github.com/virtuanista" target="_blank">virtu ğŸ£</a>
</p>

<p align="center">
    <img src="https://open.soniditos.com/cat_footer.svg" />
</p>

<p align="center">
    Copyright Â© 2025
</p>

<p align="center">
    <a href="/LICENSE"><img src="https://img.shields.io/static/v1.svg?style=for-the-badge&label=License&message=MIT&logoColor=d9e0ee&colorA=363a4f&colorB=b7bdf8"/></a>
</p>

*Generado con â¤ï¸ para crear documentaciÃ³n usando IA local*
